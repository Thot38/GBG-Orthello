package games;

import TournamentSystem.TSTimeStorage;
import TournamentSystem.tools.TSGameDataTransfer;
import controllers.*;
import controllers.MC.MCAgent;
import controllers.MC.MCAgentN;
import controllers.MCTS.MCTSAgentT;
//import controllers.MCTS0.MCTSAgentT0;
import controllers.MCTSExpectimax.MCTSExpectimaxAgt;
import controllers.PlayAgent.AgentState;
import controllers.TD.TDAgent;
import controllers.TD.ntuple2.NTupleFactory;
import controllers.TD.ntuple2.SarsaAgt;
import controllers.TD.ntuple2.TDNTuple2Agt;
import controllers.TD.ntuple2.TDNTuple3Agt;
import games.CFour.AlphaBetaAgent;
import games.CFour.openingBook.BookSum;
import games.Nim.BoutonAgent;
import games.Othello.BenchmarkPlayer.BenchMarkPlayer;
import games.TStats.TAggreg;
import params.*;
import tools.*;
import tools.Types.ACTIONS;

import javax.swing.*;
import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.io.PrintWriter;
import java.text.DecimalFormat;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;

/**
 * Class {@link XArenaFuncs} contains several methods to train, evaluate and measure the 
 * performance of agents. <ul>
 * <li> train:		train an agent one time for maxGameNum episodes and evaluate it with evalAgent
 * <li> multiTrain: train an agent multiple times and evaluate it with evalAgent
 * <li> compete:	one competition 'X vs. O', several episodes, measure win/tie/loose rate
 * <li> competeBoth call compete for pair (pa,opponent) in both roles, X and O  
 * <li> multiCompete: many competitions, measure win/tie/loose rate and avg. correct moves
 * <li> eval: 	(as part of the protected {@link Evaluator} elements) measure agent success
 * </ul> 
 * --- Batch methods are now in TicTacToeBatch ---
 * <p>
 * Known classes having {@link XArenaFuncs} objects as members: 
 * 		{@link Arena}, {@link XArenaButtons} 
 * 
 * @author Wolfgang Konen, TH Kï¿½ln, Nov'16
 * 
 */
public class XArenaFuncs 
{
	public  PlayAgent[] m_PlayAgents;
	private Arena m_Arena;
	protected Evaluator m_evaluatorT=null;
	protected Evaluator m_evaluatorQ=null;
//	protected Evaluator m_evaluatorM=null;
	protected String lastMsg="";
	protected int numPlayers;
	
	protected Random rand;
	protected LineChartSuccess lChart;

	private final String TAG = "[XArenaFuncs] ";

	protected DeviationWeightsChart wChart;
	/**
	 * percentiles for weight chart plot on wChart (only relevant for TDNTuple2Agt)
	 */
	double[] per = {5,25,50,75,95};

	public XArenaFuncs(Arena arena)
	{
		m_Arena = arena;
		numPlayers = arena.getGameBoard().getStateObs().getNumPlayers();
		m_PlayAgents = new PlayAgent[numPlayers];
		//m_PlayAgents[0] = new MinimaxAgent(sMinimax);
        rand = new Random(System.currentTimeMillis());
        lChart=new LineChartSuccess("Training Progress","gameNum","",true,false);
        wChart=new DeviationWeightsChart("","gameNum","",true,false);
	}
	
	/**
	 * Construct and return a new {@link PlayAgent}, based on the settings in 
	 * {@code sAgent} and {@code m_xab}. 
	 * <p>
	 * The difference to {@link #fetchAgent(int, String, XArenaButtons)} is
	 * that the  {@link PlayAgent} is a new agent (i.e. not yet trained for 
	 * trainable agents, agent state INIT). 
	 * <p>
	 * @param n			the player number
	 * @param sAgent	the string from the agent-select box
	 * @param m_xab		used only for reading parameter values from GUI members 
	 * @return			a new {@link PlayAgent} (initialized, but not yet trained)
	 * @throws IOException 
	 * 
	 * @see #fetchAgents(XArenaButtons)
	 */
	protected PlayAgent constructAgent(int n, String sAgent, XArenaButtons m_xab) throws IOException {
		PlayAgent pa = null;
		int maxGameNum=Integer.parseInt(m_xab.GameNumT.getText());
		int featmode = m_xab.tdPar[n].getFeatmode();
		
		if (sAgent.equals("TDS")) {
			Feature feat = m_xab.m_game.makeFeatureClass(m_xab.tdPar[n].getFeatmode());
			pa = new TDAgent(sAgent, new ParTD(m_xab.tdPar[n]), new ParOther(m_xab.oPar[n]), feat, maxGameNum);
		} else if (sAgent.equals("TD-Ntuple-2")) {
			try {
				XNTupleFuncs xnf = m_xab.m_game.makeXNTupleFuncs();
				NTupleFactory ntupfac = new NTupleFactory(); 
				int[][] nTuples = ntupfac.makeNTupleSet(new ParNT(m_xab.ntPar[n]), xnf);
				pa = new TDNTuple2Agt(sAgent, new ParTD(m_xab.tdPar[n]), new ParNT(m_xab.ntPar[n]), 
									  new ParOther(m_xab.oPar[n]), nTuples, xnf, maxGameNum);
			} catch (Exception e) {
				MessageBox.show(m_xab, 
						e.getMessage(), 
						"Warning", JOptionPane.WARNING_MESSAGE);
				//e.printStackTrace();
				pa=null;			
			}
		} else if (sAgent.equals("TD-Ntuple-3")) {
			try {
				XNTupleFuncs xnf = m_xab.m_game.makeXNTupleFuncs();
				NTupleFactory ntupfac = new NTupleFactory(); 
				int[][] nTuples = ntupfac.makeNTupleSet(new ParNT(m_xab.ntPar[n]), xnf);
				pa = new TDNTuple3Agt(sAgent, new ParTD(m_xab.tdPar[n]), new ParNT(m_xab.ntPar[n]), 
									  new ParOther(m_xab.oPar[n]), nTuples, xnf, maxGameNum);
			} catch (Exception e) {
				MessageBox.show(m_xab, 
						e.getMessage(), 
						"Warning", JOptionPane.WARNING_MESSAGE);
				//e.printStackTrace();
				pa=null;			
			}
		} else if (sAgent.equals("Sarsa")) {
			try {
				XNTupleFuncs xnf = m_xab.m_game.makeXNTupleFuncs();
				NTupleFactory ntupfac = new NTupleFactory(); 
				int[][] nTuples = ntupfac.makeNTupleSet(new ParNT(m_xab.ntPar[n]), xnf);
				int numOutputs = m_xab.m_game.gb.getDefaultStartState().getAllAvailableActions().size();
				pa = new SarsaAgt(sAgent, new ParTD(m_xab.tdPar[n]), new ParNT(m_xab.ntPar[n]), 
								  new ParOther(m_xab.oPar[n]), nTuples, xnf, numOutputs, maxGameNum);
			} catch (Exception e) {
				MessageBox.show(m_xab, 
						e.getMessage(), 
						"Warning", JOptionPane.WARNING_MESSAGE);
				//e.printStackTrace();
				pa=null;			
			}
		} else if (sAgent.equals("Minimax")) {
			pa = new MinimaxAgent(sAgent, new ParMaxN(m_xab.maxnParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("Max-N")) {
			pa = new MaxNAgent(sAgent, new ParMaxN(m_xab.maxnParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("Expectimax-N")) {
			pa = new ExpectimaxNAgent(sAgent, new ParMaxN(m_xab.maxnParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("Random")) {
			pa = new RandomAgent(sAgent, new ParOther(m_xab.oPar[n]));
//		} else if (sAgent.equals("MCTS0")) {
//			pa = new MCTSAgentT0(sAgent, null, new ParMCTS(m_xab.mctsParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("MCTS")) {
			pa = new MCTSAgentT(sAgent, null, new ParMCTS(m_xab.mctsParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("MCTS Expectimax")) {
			pa= new MCTSExpectimaxAgt(sAgent, new ParMCTSE(m_xab.mctseParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("Human")) {
			pa = new HumanPlayer(sAgent);
		} else if (sAgent.equals("MC")) {
			pa = new MCAgent(sAgent, new ParMC(m_xab.mcParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("MC-N")) {
			pa = new MCAgentN(sAgent, new ParMC(m_xab.mcParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("AlphaBeta")) {	// CFour only, see gui_agent_list in XArenaButtons
			AlphaBetaAgent alphaBetaStd = new AlphaBetaAgent(new BookSum());
			alphaBetaStd.resetBoard();
			alphaBetaStd.setTransPosSize(4);		// index into table
			alphaBetaStd.setBooks(true,false,true);	// use normal book and deep book dist
			alphaBetaStd.setDifficulty(42);			// search depth
			alphaBetaStd.randomizeEqualMoves(true);
			pa = alphaBetaStd;
		} else if (sAgent.equals("Bouton")) {		// Nim only, see gui_agent_list in XArenaButtons
			pa = new BoutonAgent(sAgent);
		} else if(sAgent.equals("Benchmark")) {
			pa = new BenchMarkPlayer();	
			System.out.println("pa object constructed");// Othello only, see gui_agent_list in xArenaButtons
		}
		return pa;
	}

	/**
	 * Fetch the {@link PlayAgent} vector from {@link Arena}. For agents which do 
	 * not need to be trained, construct a new one according to the selected choice
	 * and parameter settings. For agents which do need training, see, if 
	 * {@link #m_PlayAgents}[n] has already an agent of this type. 
	 * If so, return it, if not: 
	 * <ul>
	 * <li> if {@link #m_PlayAgents}[n]==null, construct a new agent and initialize 
	 *      it, but do not yet train it. 
	 * <li> else, throw a RuntimeException
	 * </ul>     
	 * @param n			the player number
	 * @param sAgent	the string from the agent-select box
	 * @param m_xab		used only for reading parameter values from GUI members 
	 * @return			the {@link PlayAgent} for the {@code n}th player
	 * @throws RuntimeException
	 * 
	 * @see #fetchAgents(XArenaButtons)
	 * @see #constructAgent(int, String, XArenaButtons)
	 */
	private PlayAgent fetchAgent(int n, String sAgent, XArenaButtons m_xab)
	{
		PlayAgent pa=null;
		int maxGameNum=Integer.parseInt(m_xab.GameNumT.getText());
		if (sAgent.equals("Minimax")) {
			pa= new MinimaxAgent(sAgent, new ParMaxN(m_xab.maxnParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("Max-N")) {
			pa= new MaxNAgent(sAgent, new ParMaxN(m_xab.maxnParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("Expectimax-N")) {
			pa = new ExpectimaxNAgent(sAgent, new ParMaxN(m_xab.maxnParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("Random")) {
			pa = new RandomAgent(sAgent, new ParOther(m_xab.oPar[n]));
//		} else if (sAgent.equals("MCTS0")) {
//			pa= new MCTSAgentT0(sAgent,null,new ParMCTS(m_xab.mctsParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("MCTS")) {
			pa = new MCTSAgentT(sAgent,null,new ParMCTS(m_xab.mctsParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("MCTS Expectimax")) {
			pa = new MCTSExpectimaxAgt(sAgent, new ParMCTSE(m_xab.mctseParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("Human")) {
			pa = new HumanPlayer(sAgent);
		} else if (sAgent.equals("MC")) {
			pa = new MCAgent(sAgent, new ParMC(m_xab.mcParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("MC-N")) {
			pa = new MCAgentN(sAgent, new ParMC(m_xab.mcParams[n]), new ParOther(m_xab.oPar[n]));
		} else if (sAgent.equals("AlphaBeta")) {	// CFour only, see gui_agent_list in XArenaButtons
			AlphaBetaAgent alphaBetaStd = new AlphaBetaAgent(new BookSum());
			alphaBetaStd.resetBoard();
			alphaBetaStd.setTransPosSize(4);		// index into table
			alphaBetaStd.setBooks(true,false,true);	// use normal book and deep book dist
			alphaBetaStd.setDifficulty(42);			// search depth
			alphaBetaStd.randomizeEqualMoves(true);
			pa = alphaBetaStd;
		} else if (sAgent.equals("Bouton")) {		// Nim only, see gui_agent_list in XArenaButtons
			pa = new BoutonAgent(sAgent);
		} else if( sAgent.equals("Benchmark")) { 	// Othello only, see gui_agent_list in XArenaButtons
			pa = new BenchMarkPlayer();
			System.out.println("pa object created");
		}else { // all the trainable agents:
			if (m_PlayAgents[n]==null) {
				if (sAgent.equals("TDS")) {
					Feature feat = m_xab.m_game.makeFeatureClass(m_xab.tdPar[n].getFeatmode());
					pa = new TDAgent(sAgent, new ParTD(m_xab.tdPar[n]), new ParOther(m_xab.oPar[n]), feat, maxGameNum);
				} else if (sAgent.equals("TD-Ntuple-2")) {
					try {
						XNTupleFuncs xnf = m_xab.m_game.makeXNTupleFuncs();
						NTupleFactory ntupfac = new NTupleFactory(); 
						int[][] nTuples = ntupfac.makeNTupleSet(new ParNT(m_xab.ntPar[n]),xnf);
						pa = new TDNTuple2Agt(sAgent, new ParTD(m_xab.tdPar[n]), new ParNT(m_xab.ntPar[n]), 
								              new ParOther(m_xab.oPar[n]), nTuples, xnf, maxGameNum);
					} catch (Exception e) {
						MessageBox.show(m_xab, 
								e.getMessage(), 
								"Warning", JOptionPane.WARNING_MESSAGE);
						//e.printStackTrace();
						pa=null;			
					}
				} else if (sAgent.equals("TD-Ntuple-3")) {
					try {
						XNTupleFuncs xnf = m_xab.m_game.makeXNTupleFuncs();
						NTupleFactory ntupfac = new NTupleFactory(); 
						int[][] nTuples = ntupfac.makeNTupleSet(new ParNT(m_xab.ntPar[n]),xnf);
						pa = new TDNTuple3Agt(sAgent, new ParTD(m_xab.tdPar[n]), new ParNT(m_xab.ntPar[n]), 
								              new ParOther(m_xab.oPar[n]), nTuples, xnf, maxGameNum);
					} catch (Exception e) {
						MessageBox.show(m_xab, 
								e.getMessage(), 
								"Warning", JOptionPane.WARNING_MESSAGE);
						//e.printStackTrace();
						pa=null;			
					}
				} else if (sAgent.equals("Sarsa")) {
					try {
						XNTupleFuncs xnf = m_xab.m_game.makeXNTupleFuncs();
						NTupleFactory ntupfac = new NTupleFactory(); 
						int[][] nTuples = ntupfac.makeNTupleSet(new ParNT(m_xab.ntPar[n]),xnf);
						int numOutputs = m_xab.m_game.gb.getDefaultStartState().getAllAvailableActions().size();
						pa = new SarsaAgt(sAgent, new ParTD(m_xab.tdPar[n]), new ParNT(m_xab.ntPar[n]), 
										  new ParOther(m_xab.oPar[n]), nTuples, xnf, numOutputs, maxGameNum);
					} catch (Exception e) {
						MessageBox.show(m_xab, 
								e.getMessage(), 
								"Warning Sarsa", JOptionPane.WARNING_MESSAGE);
						//e.printStackTrace();
						pa=null;			
					}
				}					
			} else {   // i.e. if m_PlayAgents[n]!=null
				// --- questionable, the code concerned with wrapper!! ---
				PlayAgent inner_pa = m_PlayAgents[n];
				if (m_PlayAgents[n].getName()=="ExpectimaxWrapper") 
					inner_pa = ((ExpectimaxWrapper) inner_pa).getWrappedPlayAgent();
				if (!sAgent.equals(inner_pa.getName()))
					throw new RuntimeException("Current agent for player "+n+" is "+m_PlayAgents[n].getName()
							+" but selector for player "+n+" requires "+sAgent+".");
				pa = m_PlayAgents[n];		// take the n'th current agent, which 
											// is *assumed* to be trained (!)
			}
		} 
		if (pa==null) 
			throw new RuntimeException("Could not construct/fetch agent = "+sAgent);
		
		return pa;
	}
	
	/**
	 * Fetch the vector of all {@link PlayAgent}s from {@link Arena}. See 
	 * {@link #fetchAgent(int, String, XArenaButtons)} for the rules how to fetch.
	 *      
	 * @param m_xab where to read the settings from
	 * @return the vector {@code m_PlayAgents} of all agents in the arena
	 * @throws RuntimeException
	 * 
	 * @see #constructAgent(int, String, XArenaButtons)
	 * @see #fetchAgent(int, String, XArenaButtons)
	 */
	protected PlayAgent[] fetchAgents(XArenaButtons m_xab) 
			throws RuntimeException
	{
		if (m_PlayAgents==null) m_PlayAgents=new PlayAgent[numPlayers];
		PlayAgent pa=null;
		for (int n=0; n<numPlayers; n++) {
			String sAgent = m_xab.getSelectedAgent(n);
			pa = fetchAgent(n, sAgent, m_xab);
			m_PlayAgents[n] = pa;
		} 
		return m_PlayAgents;
	}

	/**
	 * Given the selected agents in {@code paVector}, do nothing if their {@code nply==0}. 
	 * But if their {@code nply>0}, wrap them by an n-ply look-ahead tree search. 
	 * The tree is of type Max-N for deterministic games and of type
	 * Expectimax-N for nondeterministic games. No wrapping occurs for agent {@link HumanPlayer}.
	 * <p>
	 * Caution: Larger values for {@code nply}, e.g. greater 5, may lead to long execution times 
	 * and large memory requirements!
	 * 
	 * @param paVector	the (unwrapped) agents for each player 
	 * @param m_xab		for access to m_xab.oPar, the vector of {@link OtherParams}, 
	 * 					containing {@code nply = oPar[n].getWrapperNPly()} for 
	 * 					each agent separately. And for access to m_xab.maxnParams, the vector of
	 * 					{@link MaxNParams} containing {@code boolean maxNHashMap}.
	 * @param so		needed only to detect whether game is deterministic or not.
	 * @return a vector of agents ({@code paVector} itself if {@code nply==0}; wrapped agents 
	 * 					if {@code nply>0})
	 * 
	 * @see MaxNWrapper
	 * @see ExpectimaxWrapper
	 */
	protected PlayAgent[] wrapAgents(PlayAgent[] paVector, XArenaButtons m_xab, StateObservation so) 
//	protected PlayAgent[] wrapAgents(PlayAgent[] paVector, OtherParams[] oPar, StateObservation so) 
	{
		// avt1, avt2, avt3 are only for (temporary) debug info
//		Types.ACTIONS_VT avt1 = paVector[0].getNextAction2(so,false,true);
		PlayAgent[] qaVector = new PlayAgent[numPlayers];
		for (int n=0; n<numPlayers; n++) {
			qaVector[n] = wrapAgent(n, paVector[n], new ParOther(m_xab.oPar[n])
												  , new ParMaxN(m_xab.maxnParams[n]), so);
		} // for (n)
//		Types.ACTIONS_VT avt2 = qaVector[0].getNextAction2(so,false,true);
//		PlayAgent qa = ((MaxNWrapper)qaVector[0]).getWrappedPlayAgent();
//		Types.ACTIONS_VT avt3 = qa.getNextAction2(so,false,true);
		return qaVector;
	}

	// a similar function, just needed by TS
	protected PlayAgent[] wrapAgents(PlayAgent[] paVector, StateObservation so, XArenaButtons m_xab) 
	{
		PlayAgent[] qaVector = new PlayAgent[numPlayers];
		for (int n=0; n<numPlayers; n++) {
			qaVector[n] = wrapAgent(n, paVector[n], paVector[n].getParOther()
												  , new ParMaxN(m_xab.maxnParams[n]), so);
		} // for (n)
		return qaVector;
	}

	protected PlayAgent wrapAgent(int n, PlayAgent pa, ParOther oPar, ParMaxN mPar, StateObservation so) 
	{
		PlayAgent qa;
		int nply = oPar.getWrapperNPly();
		mPar.setMaxNDepth(nply);
		if (nply>0 && !(pa instanceof HumanPlayer)) {
			if (so.isDeterministicGame()) {
				qa = new MaxNWrapper(pa,mPar,oPar);		// mPar has useMaxNHashMap
//				qa = new MaxNWrapper(pa,nply);			// always maxNHashMap==false
			} else {
				qa = new ExpectimaxWrapper(pa,nply);
			}
		} else {
			qa=pa;			
		}
		return qa;
	}


	/**
	 * Perform one training of a {@link PlayAgent} sAgent with maxGameNum episodes. 
	 * @param n			index of agent to train
	 * @param sAgent	a string containing the class name of the agent
	 * @param xab		used only for reading parameter values from members td_par, cma_par
	 * @param gb		the game board
	 * @return	the trained PlayAgent
	 * @throws IOException 
	 */
	public PlayAgent train(int n, String sAgent, XArenaButtons xab, GameBoard gb) throws IOException {
		int stopTest;			// 0: do not call Evaluator during training; 
								// >0: call Evaluator after every stopTest training games
		int stopEval;			// 0: do not stop on Evaluator; 
								// >0: stop, if Evaluator stays true for stopEval games
		int maxGameNum;			// maximum number of training games
		int numEval;			// evaluate the trained agent every numEval games
		boolean learnFromRM;	// if true, learn from random moves during training
		int gameNum=0;
		int verbose=2;

		maxGameNum = Integer.parseInt(xab.GameNumT.getText());
		numEval = xab.oPar[n].getNumEval();
		if (numEval==0) numEval=500; // just for safety, to avoid ArithmeticException in 'gameNum%numEval' below

		boolean doTrainStatistics=true;
		ArrayList tsList = new ArrayList<TStats>();
		ArrayList taggList = new ArrayList<TAggreg>();

		DecimalFormat frm = new DecimalFormat("#0.0000");
		PlayAgent pa = null;
		PlayAgent qa = null;

		try {
			pa = this.constructAgent(n,sAgent, xab);
			if (pa==null) throw new RuntimeException("Could not construct agent = " + sAgent);
			
		}  catch(RuntimeException e) {
			MessageBox.show(xab, 
					e.getMessage(), 
					"Warning", JOptionPane.WARNING_MESSAGE);
			return pa;			
		} 
		
		String pa_string = pa.getClass().getName();
		if (!pa.isTrainable()) {
			System.out.println(pa_string + " is not trainable");
			return pa;
		}
			
		
		// initialization weight distribution plot:
		int plotWeightMode = xab.ntPar[n].getPlotWeightMethod();
		wChart.initializeChartPlot(xab,pa,plotWeightMode);
		
		System.out.println(pa.stringDescr());
		System.out.println(pa.stringDescr2());
		pa.setMaxGameNum(maxGameNum);
		pa.setNumEval(numEval);
		pa.setGameNum(0);
		System.out.println(pa.printTrainStatus());
		
		stopTest = xab.oPar[n].getStopTest();
		stopEval = xab.oPar[n].getStopEval();
		learnFromRM = xab.oPar[n].useLearnFromRM();
		int qem = xab.oPar[n].getQuickEvalMode();
        m_evaluatorQ = xab.m_game.makeEvaluator(pa,gb,stopEval,qem,1);
		int tem = xab.oPar[n].getTrainEvalMode();
		//
		// doTrainEvaluation flags whether Train Evaluator is executed:
		// Evaluator m_evaluatorT is only constructed and evaluated, if in tab 'Other pars' 
		// the choice box 'Train Eval Mode' is not -1 ("none").
		boolean doTrainEvaluation = (tem!=-1);
		if (doTrainEvaluation) {
	        m_evaluatorT = xab.m_game.makeEvaluator(pa,gb,stopEval,tem,1);
		}

		// initialization line chart plot:
		lChart.initializeChartPlot(xab,m_evaluatorQ,doTrainEvaluation);

		// Debug only: direct debug output to file debug.txt
		//TDNTupleAgt.pstream = System.out;
		//TDNTupleAgt.pstream = new PrintStream(new FileOutputStream("debug-TDNT.txt"));
		
		//{
		
		long startTime = System.currentTimeMillis();
		gb.initialize();
		while (pa.getGameNum()<pa.getMaxGameNum())
		{		
			StateObservation so = soSelectStartState(gb,xab.oPar[n].useChooseStart01(), pa); 

			// --- only debug TTT -----
//			so.advance(new ACTIONS(4));
//			so.advance(new ACTIONS(5));
//			so.advance(new ACTIONS(8));
//			so.advance(new ACTIONS(7));
			
			pa.trainAgent(so);
			
			if (doTrainStatistics) collectTrainStats(tsList,pa,so);
				
			gameNum = pa.getGameNum();
			if (gameNum%numEval==0 ) { //|| gameNum==1) {
				double elapsedTime = (double)(System.currentTimeMillis() - startTime)/1000.0;
				System.out.println(pa.printTrainStatus()+", "+elapsedTime+" sec");
				xab.GameNumT.setText(Integer.toString(gameNum ) );
				
				// construct 'qa' anew (possibly wrapped agent for eval)
				qa = wrapAgent(n, pa, new ParOther(xab.oPar[n]), new ParMaxN(xab.maxnParams[n]), gb.getStateObs());

		        m_evaluatorQ.eval(qa);
				if (doTrainEvaluation)
					m_evaluatorT.eval(qa);

				// update line chart plot:
				lChart.updateChartPlot(gameNum, m_evaluatorQ, m_evaluatorT, doTrainEvaluation, false);

				// update weight / TC factor distribution plot:
				wChart.updateChartPlot(gameNum,pa,per);

				// enable premature exit if TRAIN button is pressed again:
				if (xab.m_game.taskState!=Arena.Task.TRAIN) {
					MessageBox.show(xab,
							"Training stopped prematurely",
							"Warning", JOptionPane.WARNING_MESSAGE);
					break; //out of while
				}

				startTime = System.currentTimeMillis();
			}
			
			if (stopTest>0 && (gameNum-1)%numEval==0 && stopEval>0) {
				// construct 'qa' anew (possibly wrapped agent for eval)
				qa = wrapAgent(n, pa, new ParOther(xab.oPar[n]), new ParMaxN(xab.maxnParams[n]), gb.getStateObs());
		        
				if (doTrainEvaluation) {
					m_evaluatorT.eval(qa);
					m_evaluatorT.goalReached(gameNum);
				}
				
				m_evaluatorQ.eval(qa); 
				if(m_evaluatorQ.goalReached(gameNum)) break;  // out of while
				
			}
		} // while
		
		// Debug only
		//TDNTupleAgt.pstream.close();
			
		//} // if(sAgent)..else
		
		if (doTrainStatistics) {
			taggList = aggregateTrainStats(tsList);
			System.out.println("--- Train Statistics ---");
			TStats.printTAggregList(taggList);
		}
		
		//xab.GameNumT.setText(Integer.toString(maxGameNum) );		// restore initial value (maxGameNum)
														// not sensible in case of premature stop;
														// and in case of normal end, it will be maxGameNum anyhow

		//samine
		int test=2000;
		if (gameNum%test!=0) 
			System.out.println(pa.printTrainStatus());

//-- only debug
//		m_evaluator2.eval(); 
//        Evaluator2 m_evaluator2New = new Evaluator2(pa,0,2);
//        m_evaluator2New.eval();
		if (stopTest>0 && stopEval>0) {
			System.out.println(m_evaluatorQ.getGoalMsg(gameNum));
			if (doTrainEvaluation) System.out.println(m_evaluatorT.getGoalMsg(gameNum));
		}
		
		System.out.println("final "+m_evaluatorQ.getMsg());
		if (doTrainEvaluation && m_evaluatorT.getMsg()!=null) System.out.println("final "+m_evaluatorT.getMsg());
								// getMsg() might be null if evaluator mode = -1 (no evaluation)

		return pa;
	}
	
	private StateObservation soSelectStartState(GameBoard gb, boolean chooseStart01, PlayAgent pa) {
		StateObservation so; 
		if (chooseStart01) {
			so = gb.chooseStartState(pa);
		} else {
			so = gb.getDefaultStartState();  
		}					
		return so;
	}
	
	private void collectTrainStats(ArrayList<TStats> tsList, PlayAgent pa, StateObservation so) {
		int n=pa.getGameNum();
		int p=so.getMinEpisodeLength();
		int moveNum=pa.getMoveCounter();
		int epiLength = pa.getParOther().getEpisodeLength();
        TStats tstats = new TStats(n,p,moveNum,epiLength);
        tsList.add(tstats);
	}
	
	private ArrayList<TAggreg> aggregateTrainStats(ArrayList<TStats> tsList) {
		ArrayList taggList = new ArrayList<TAggreg>();
		TStats tstats;
		TAggreg tagg;
		HashSet pvalues = new HashSet();
		Iterator it = tsList.iterator();
	    while (it.hasNext()) {
		    tstats = (TStats)it.next();
	    	pvalues.add(tstats.p);
	    }
		Iterator itp = pvalues.iterator();
	    while (itp.hasNext()) {
	    	int p=(int)itp.next();
 			tagg = new TAggreg(tsList,p);
 			taggList.add(tagg);
	    }
		return taggList;
	}

	/**
	 * Perform trainNum cycles of training and evaluation for PlayAgent, each 
	 * training with maxGameNum games. Record results in {@code multiTrain.csv}, see below.
	 * 
	 * @param n			index of agent to train (the current GUI will call multiTrain 
	 * 					always with n=0 
	 * @param sAgent	a string containing the class name of the agent
	 * @param xab		used only for reading parameter values from members td_par, cma_par
	 * @return the (last) trained agent
	 * @throws IOException if something goes wrong with {@code multiTrain.csv}, see below
	 * <p>
	 * Side effect: writes results of multi-training to <b>{@code agents/<gameDir>/csv/multiTrain.csv}</b>.
	 * This file has the columns: <br>
	 * {@code run, gameNum, evalQ, evalT, evalM, actionNum, trnMoves}. <br>
	 * The contents may be visualized with one of the R-scripts in {@code resources\R_plotTools}.
	 */
	public PlayAgent multiTrain(int n, String sAgent, XArenaButtons xab, GameBoard gb) throws IOException {
		DecimalFormat frm3 = new DecimalFormat("+0.000;-0.000");
		DecimalFormat frm = new DecimalFormat("#0.000");
		DecimalFormat frm2 = new DecimalFormat("+0.00;-0.00");
		DecimalFormat frm1 = new DecimalFormat("#0.00");
		int verbose=1;
		int stopEval = 0;

		int trainNum=Integer.valueOf(xab.TrainNumT.getText()).intValue();
		int maxGameNum=Integer.parseInt(xab.GameNumT.getText());
		boolean learnFromRM = xab.oPar[n].useLearnFromRM();
		PlayAgent pa = null, qa= null;
		
		boolean doTrainEvaluation=false;
		boolean doMultiEvaluation=false;

		System.out.println("*** Starting multiTrain with trainNum = "+trainNum+" ***");

		Measure oQ = new Measure();			// quick eval measure
		Measure oT = new Measure();			// train eval measure
//		Measure oM = new Measure();			// multiTrain eval measure
		MTrain mTrain;
		double evalQ=0.0, evalT=0.0, evalM=0.0;
		double elapsedTime=0.0, numEvalTime;
		ArrayList<MTrain> mtList = new ArrayList<MTrain>();
		int maxGameNumV=10000;
		
		for (int i=0; i<trainNum; i++) {
			xab.TrainNumT.setText(Integer.toString(i+1)+"/"+Integer.toString(trainNum) );

			try {
				pa = constructAgent(n,sAgent, xab);
				if (pa==null) throw new RuntimeException("Could not construct AgentX = " + sAgent);				
			}  catch(RuntimeException e) 
			{
				MessageBox.show(xab, 
						e.getMessage(), 
						"Warning", JOptionPane.WARNING_MESSAGE);
				return pa;			
			} 


			int qem = xab.oPar[n].getQuickEvalMode();
	        m_evaluatorQ = xab.m_game.makeEvaluator(pa,gb,stopEval,qem,1);
			int tem = xab.oPar[n].getTrainEvalMode();
			//
			// doTrainEvaluation flags whether Train Evaluator is executed:
			// Evaluator m_evaluatorT is only constructed and evaluated, if the choice
			// boxes 'Quick Eval Mode' and 'Train Eval Mode' in tab 'Other pars' have
			// different values. 
			doTrainEvaluation = (tem!=qem);
			if (doTrainEvaluation)
		        m_evaluatorT = xab.m_game.makeEvaluator(pa,gb,stopEval,tem,1);
			
//			int mem = m_evaluatorQ.getMultiTrainEvalMode();
//			//
//			// doMultiEvaluation flags whether Multi Train Evaluator is executed:
//			// Evaluator m_evaluatorM is only constructed and evaluated, if the value of
//			// getMultiTrainEvalMode() differs from the values in choice boxes
//			// 'Quick Eval Mode' and 'Train Eval Mode' in tab 'Other pars' . 
//			doMultiEvaluation = ((mem!=qem) && (mem!=tem));
//			if (doMultiEvaluation)
//		        m_evaluatorM = xab.m_game.makeEvaluator(pa,gb,stopEval,mem,1);

			if (i==0) {
				String pa_string = pa.getClass().getName();
				System.out.println(pa.stringDescr());
				System.out.println(pa.stringDescr2());
			}
			pa.setMaxGameNum(maxGameNum);
			pa.setGameNum(0);
			int player; 
			int numEval = xab.oPar[n].getNumEval();
			int gameNum;
			long actionNum, trnMoveNum;
			PlayAgent[] paVector;
			
			{
				long startTime = System.currentTimeMillis();
				long startTime0 = startTime;
				gb.initialize();
				while (pa.getGameNum()<pa.getMaxGameNum())
				{		
					StateObservation so = soSelectStartState(gb,xab.oPar[n].useChooseStart01(), pa); 

					pa.trainAgent(so);
					
					gameNum = pa.getGameNum();
					if (gameNum%numEval==0 ) { //|| gameNum==1) {
						numEvalTime = (double)(System.currentTimeMillis() - startTime)/1000.0;
						// numEvalTime: the time in sec for the last numEval training games
						System.out.println(pa.printTrainStatus()+", "+numEvalTime+" sec");
						xab.GameNumT.setText(Integer.toString(gameNum ) );
						
						// construct 'qa' anew (possibly wrapped agent for eval)
						qa = wrapAgent(n, pa, new ParOther(xab.oPar[n]), new ParMaxN(xab.maxnParams[n]), gb.getStateObs());
				        
						m_evaluatorQ.eval(qa);
						evalQ = m_evaluatorQ.getLastResult();
						if (doTrainEvaluation) {
							m_evaluatorT.eval(qa);
							evalT = m_evaluatorT.getLastResult();
						}
//						if (doMultiEvaluation) {
//							m_evaluatorM.eval(qa);
//							evalM = m_evaluatorM.getLastResult();
//						}
						
                        // gather information for later printout to agents/gameName/csv/multiTrain.csv.
						actionNum = pa.getNumLrnActions();	
						trnMoveNum = pa.getNumTrnMoves();
						elapsedTime += numEvalTime;   	// the time needed since start of training 
													 	//(only self-play, excluding evaluations)
						mTrain = new MTrain(i,gameNum,evalQ,evalT,/*evalM,*/
											actionNum,trnMoveNum,elapsedTime,actionNum/elapsedTime);
						mtList.add(mTrain);

						// enable premature exit if MULTITRAIN button is pressed again:
						if (xab.m_game.taskState!=Arena.Task.MULTTRN) {
							MessageBox.show(xab,
									"MultiTraining stopped prematurely",
									"Warning", JOptionPane.WARNING_MESSAGE);
							break; //out of while
						}

						startTime = System.currentTimeMillis();
					}
				}
				
			} // if(sAgent)..else
			
			// construct 'qa' anew (possibly wrapped agent for eval)
			qa = wrapAgent(0, pa, new ParOther(xab.oPar[n]), new ParMaxN(xab.maxnParams[n]), gb.getStateObs());

	        // evaluate again at the end of a training run:
			m_evaluatorQ.eval(qa);
			oQ.add(m_evaluatorQ.getLastResult());
			if (doTrainEvaluation) {
				m_evaluatorT.eval(qa);
				oT.add(m_evaluatorT.getLastResult());								
			}
//			if (doMultiEvaluation) {
//				m_evaluatorM.eval(qa);
//				oM.add(m_evaluatorM.getLastResult());
//			}
			
			if (xab.m_game.taskState!=Arena.Task.MULTTRN) {
				break; //out of for
			}
		} // for (i)
		if (m_evaluatorQ.m_mode!=(-1)) 
			// m_mode=-1 signals: 'no evaluation done' --> oT did not receive evaluation results
		{
			System.out.println("Avg. "+ m_evaluatorQ.getPrintString()+frm3.format(oQ.getMean()) + " +- " + frm.format(oQ.getStd()));
		}
		if (doTrainEvaluation && m_evaluatorT.m_mode!=(-1)) 
								 // m_mode=-1 signals: 'no evaluation done' --> oT did not receive evaluation results
		{
		  System.out.println("Avg. "+ m_evaluatorT.getPrintString()+frm3.format(oT.getMean()) + " +- " + frm.format(oT.getStd()));
		}
//		if (doMultiEvaluation)
//		  System.out.println("Avg. "+ m_evaluatorM.getPrintString()+frm3.format(oM.getMean()) + " +- " + frm.format(oM.getStd()));
		if (m_evaluatorQ.m_mode==(-1)) {
			this.lastMsg = "Warning: No evaluation done (Quick Eval Mode = -1)";
		} else {
			this.lastMsg = (m_evaluatorQ.getPrintString() + frm2.format(oQ.getMean()) + " +- " + frm1.format(oQ.getStd()) + "");			
		}
		
		MTrain.printMultiTrainList(mtList, pa, m_Arena);
		
		xab.TrainNumT.setText(Integer.toString(trainNum) );
		return pa;
		
	} // multiTrain

	/**
	 * Test player pa by playing competeNum games against opponent, both as X and as O.
	 * Start each game with an empty board.
	 * @param pa		a trained agent
	 * @param opponent	a trained agent
	 * @param competeNum
	 * @param verbose TODO
	 * @param gb		needed to get a default start state
	 * @return the fitness of pa, which is +1 if pa always wins, 0 if always tie or if #win=#loose
	 *         and -1 if pa always looses.  
	 * 
	 * @see XArenaButtons
	 */
	public static double competeBoth(PlayAgent pa, PlayAgent opponent, StateObservation startSO,
									 int competeNum, int verbose, GameBoard gb) {
		double[] res;
		double resX, resO;

		// now passed as parameter
		//StateObservation startSO = gb.getDefaultStartState();  // empty board

		res = XArenaFuncs.compete(pa, opponent, startSO, competeNum, verbose, null);
		resX  = res[0] - res[2];		// X-win minus O-win percentage, \in [-1,1]
										// resp. \in [-1,0], if opponent never looses.
										// +1 is best for pa, -1 worst for pa.
		res = XArenaFuncs.compete(opponent, pa, startSO, competeNum, verbose, null);
		resO  = res[2] - res[0];		// O-win minus X-win percentage, \in [-1,1]
										// resp. \in [-1,0], if opponent never looses.
										// +1 is best for pa, -1 worst for pa.
		return (resX+resO)/2.0;
	}
	
	/**
	 * Perform a competition paX vs. paO consisting of competeNum games, starting from StateObservation startSO.
	 * @param paX	PlayAgent,	a trained agent
	 * @param paO	PlayAgent,	a trained agent
	 * @param startSO	the start board position for the game
	 * @param competeNum		the number of episodes to play
	 * @param verbose			0: silent, 1,2: more print-out
	 * @param nextTimes storage to save time measurements, null if not needed (currently only used 
	 * 				by tournament system). If {@code nextTimes} is not null, some extra info for the tournament
	 *              log is printed to {@code System.out}.
	 * @return	double[3], the percentage of episodes with X-win, tie, O-win
	 */
	public static double[] compete(PlayAgent paX, PlayAgent paO, StateObservation startSO, 
			int competeNum, int verbose, TSTimeStorage[] nextTimes) {
		double[] winrate = new double[3];
		int xwinCount=0, owinCount=0, tieCount=0;
		double moveCount = 0.0;
		DecimalFormat frm = new DecimalFormat("#0.000");
		boolean nextMoveSilent = (verbose<2 ? true : false);
		StateObservation so;
		Types.ACTIONS actBest;
		
		String paX_string = paX.stringDescr();
		String paO_string = paO.stringDescr();
		if (verbose>0) System.out.println("Competition, "+competeNum+" episodes: \n"
				+"      "+paX_string + "\n"
				+"   vs "+paO_string);
		
		if (nextTimes != null) {
			// some diagnostic info to print when called via tournament system:
			System.out.println("Competition: "+competeNum+" episodes "+paX_string+" vs "+paO_string/*+" with "+rndmStartMoves+" random startmoves"*/);
			String currDateTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd--HH.mm.ss"));
			System.out.println("Episode Start @ "+currDateTime);
			System.out.println("start state: "+startSO);			
		}

		for (int k=0; k<competeNum; k++) {
			int Player = Types.PLAYER_PM[startSO.getPlayer()];			
			so = startSO.copy();

			while(true)
			{

				if(Player==1){		// make a X-move
//					int n=so.getNumAvailableActions();
					long startTNano = System.nanoTime();
					actBest = paX.getNextAction2(so, false, nextMoveSilent);
					long endTNano = System.nanoTime();
					if (nextTimes!=null) nextTimes[0].addNewTimeNS(endTNano-startTNano);
					so.advance(actBest);
					Player=-1;
				}
				else				// i.e. O-Move
				{
//					int n=so.getNumAvailableActions();
					long startTNano = System.nanoTime();
					actBest = paO.getNextAction2(so, false, nextMoveSilent);
					long endTNano = System.nanoTime();
					if (nextTimes!=null) nextTimes[1].addNewTimeNS(endTNano-startTNano);
					so.advance(actBest);
					Player=+1;
				}
				if (so.isGameOver()) {
					int res = so.getGameWinner().toInt();
					moveCount += so.getMoveCounter();
					//  res is +1/0/-1  for X/tie/O win
					int player = Types.PLAYER_PM[so.getPlayer()];
					switch (res*player) {
						case -1:
							if (verbose>0) System.out.println(k+": O wins");
							owinCount++;
							break;
						case 0:
							if (verbose>0) System.out.println(k+": Tie");
							tieCount++;
							break;
						case +1:
							if (verbose>0) System.out.println(k+": X wins");
							xwinCount++;
							break;
					}

					break; // out of while

				} // if (so.isGameOver())
			}	// while(true)

		} // for (k)
		winrate[0] = (double)xwinCount/competeNum;
		winrate[1] = (double) tieCount/competeNum;
		winrate[2] = (double)owinCount/competeNum;
		moveCount /= competeNum;

		if (verbose>0) {
			if (verbose>1) {
				System.out.print("win rates: ");
				for (int i=0; i<3; i++) System.out.print(frm.format(winrate[i])+"  ");
				System.out.println(" (X/Tie/O)");				
			}
			System.out.println("Avg # moves in "+ competeNum +" episodes = "+ frm.format(moveCount));			
		}

		return winrate;
	} // compete

	// --- no longer needed, use XArenaFuncs.compete(...,TSTimeStorage[] nextTimes) instead ---
//	/**
//	 * This is an adapted version of {@link XArenaFuncs#compete(PlayAgent, PlayAgent, StateObservation, int, int, TSTimeStorage[]) XArenaFuncs.compete()}. 
//	 * The adaption is for the tournament system: the tournament parameters are added.
//	 * <p>
//	 * -- no longer needed, use {@link XArenaFuncs#compete(PlayAgent, PlayAgent, StateObservation, int, int, TSTimeStorage[]) XArenaFuncs.compete()} instead. --
//	 * @param paX PlayAgent, a trained agent
//	 * @param paO PlayAgent, a trained agent
//	 * @param startSO    the start board position for the game
//	 * @param competeNum the number of episodes to play (always 1)
//	 * @param nextTimes time storage to save measurements
//	 * @return double[3], the percentage of episodes with X-win, tie, O-win
//	 */
//	// @param rndmStartMoves -- seems obsolete now /WK/2019/04/ ---
//	@Deprecated
//	public static double[] competeTS(PlayAgent paX, PlayAgent paO, StateObservation startSO, 
//			int competeNum, int verbose, TSTimeStorage[] nextTimes /*, int rndmStartMoves*/) {
//		double[] winrate = new double[3];
//		int xwinCount=0, owinCount=0, tieCount=0;
//		DecimalFormat frm = new DecimalFormat("#0.000");
//		boolean nextMoveSilent = (verbose<2 ? true : false);
//		StateObservation so;
//		Types.ACTIONS actBest;
//
//		String paX_string = paX.stringDescr();
//		String paO_string = paO.stringDescr();
//		System.out.println("Competition: "+competeNum+" episodes "+paX_string+" vs "+paO_string/*+" with "+rndmStartMoves+" random startmoves"*/);
//		/*
//		if (rndmStartMoves>0) {
//			RandomAgent raX = new RandomAgent("Random Agent X");
//			//RandomAgent raO = new RandomAgent("Random Agent O");
//			for (int n = 0; n < rndmStartMoves; n++) {
//				startSO.advance(raX.getNextAction2(startSO, false, true));
//				//startSO.advance(raO.getNextAction2(startSO, false, true));
//			}
//			System.out.println("RandomStartState: "+startSO);
//		}
//		*/
//		
//		// --- this part specific to competeTS() ---
//		String currDateTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd--HH.mm.ss"));
//		System.out.println("Episode Start @ "+currDateTime);
//		System.out.println("start state: "+startSO);
//
//		for (int k=0; k<competeNum; k++) { // ist im TS immer 1
//			int Player = Types.PLAYER_PM[startSO.getPlayer()];
//			so = startSO.copy();
//
//			//ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean();
//			//System.out.println("threadMXBean.isCurrentThreadCpuTimeSupported() "+threadMXBean.isCurrentThreadCpuTimeSupported());
//
//			while(true) {
//				System.out.print("#"); // simple progressbar so user knows something is happening
//				if(Player==1){		// make a X-move
//					int n = so.getNumAvailableActions();
//
//					//long startT = System.currentTimeMillis();
//					//StopWatch stopwatch = new StopWatch(); // teil des apache common lang3 pakets - verwendet intern aber auch nur System.nanoTime()
//					//stopwatch.start();
//					long startTNano = System.nanoTime();
//					//long startTNanoThread = threadMXBean.getCurrentThreadCpuTime();
//					//long startTNanoInstant = Instant.now().getNano();
//					//long startTNanoInstant = Instant.now().toEpochMilli();
//
//					actBest = paX.getNextAction2(so, false, nextMoveSilent); // agent moves!
//					//long endT = System.currentTimeMillis();
//					//stopwatch.stop();
//					long endTNano = System.nanoTime();
//					//long endTNanoThread = threadMXBean.getCurrentThreadCpuTime();
//					//long endTNanoInstant = Instant.now().getNano(); // ca selbe ergebnis wie System.nanoTime
//					//long endTNanoInstant = Instant.now().toEpochMilli(); // bei zeiten <1ms eigentlich immer 0
//					// Debug Printlines
//					//System.out.println("paX.getNextAction2(so, false, true); processTime: "+(endT-startT)+"ms");
//					//System.out.println("paX.getNextAction2(so, false, true); processTime: "+(endTNano-startTNano)+"ns | "+(endTNano-startTNano)/(1*Math.pow(10,6))+"ms (aus ns)");
//					nextTimes[0].addNewTimeNS(endTNano-startTNano);
//					//System.out.println("Time nanoTime:::: "+ (endTNano-startTNano)+"ns = "+ timeDiffNStoMS(startTNano,endTNano)+"ms");
//					//System.out.println("Time ThreadNano:: "+ (endTNanoThread-startTNanoThread)+"ns = "+ timeDiffNStoMS(startTNanoThread,endTNanoThread)+"ms");
//					//System.out.println("Time InstantNano: "+ (endTNanoInstant-startTNanoInstant)+"ns = "+ timeDiffNStoMS(startTNanoInstant,endTNanoInstant)+"ms");
//					//System.out.println("Time StWatchNano: "+ (stopwatch.getNanoTime())+"ns = "+ timeNStoMS(stopwatch.getNanoTime())+"ms");
//					//System.out.println("-----------------");
//
//					so.advance(actBest);
//					Player = -1;
//				}
//				else				// i.e. O-Move
//				{
//					int n = so.getNumAvailableActions();
//
//					//long startT = System.currentTimeMillis();
//					long startTNano = System.nanoTime();
//					actBest = paO.getNextAction2(so, false, nextMoveSilent); // agent moves!
//					//long endT = System.currentTimeMillis();
//					long endTNano = System.nanoTime();
//					// Debug Printlines
//					//System.out.println("paO.getNextAction2(so, false, true); processTime: "+(endT-startT)+"ms");
//					//System.out.println("paO.getNextAction2(so, false, true); processTime: "+(endTNano-startTNano)+"ns | "+(endTNano-startTNano)/(1*Math.pow(10,6))+"ms (aus ns)");
//					nextTimes[1].addNewTimeNS(endTNano-startTNano);
//
//					so.advance(actBest);
//					Player = +1;
//				}
//				if (so.isGameOver()) {
//					System.out.println(); // make new line to not put text at end of ##### progressbar (1st line of while)
//					int res = so.getGameWinner().toInt();
//					//  res is +1/0/-1  for X/tie/O win
//					int player = Types.PLAYER_PM[so.getPlayer()];
//					switch (res*player) {
//					case -1:
//						System.out.println(k+": O wins");
//						owinCount++;
//						break;
//					case 0:
//						System.out.println(k+": Tie");
//						tieCount++;
//						break;
//					case +1:
//						System.out.println(k+": X wins");
//						xwinCount++;
//						break;
//					}
//
//					break; // out of while
//
//				} // if (so.isGameOver())
//			}	// while(true)
//
//		} // for (k)
//		winrate[0] = (double)xwinCount/competeNum;
//		winrate[1] = (double) tieCount/competeNum;
//		winrate[2] = (double)owinCount/competeNum;
//
//		System.out.print("win rates: ");
//		for (int i=0; i<3; i++)
//			System.out.print(frm.format(winrate[i])+"  ");
//		System.out.println(" (X/Tie/O)");
//
//		return winrate;
//	} // competeTS

	// --- seems obsolete now, see TSTimeStorage ---
//	private static double timeDiffNStoMS(long startT, long endT) {
//		double s = startT;
//		double e = endT;
//		double r = e - s;
//		return r/1000000;
//	}
//	private static double timeNStoMS(long startT) {
//		double r = startT;
//		return r/1000000;
//	}
	
	/**
	 * Does the main work for menu items 'Single Compete', 'Swap Compete' and 'Compete Both'.
	 * These menu items set enum {@link Arena#taskState} to either COMPETE or SWAPCMP or BOTHCMP.
	 * Then the appropriate cases of {@code switch} in Arena.run() will call competeBase. 
	 * <p>
	 * 'Single Compete' performs {@code competeNum} competitions AgentX as X vs. AgentO as O. 
	 * 'Swap Compete' performs competeNum competitions AgentX as O vs. AgentO as X. 
	 * 'Compete Both' combines 'Compete' and 'Swap Compete'.
	 * <p>
	 * The agents AgentX and AgentO are fetched from {@code xab} and are assumed to be
	 * trained (!). The parameters for X and O are fetched from the param tabs. The parameter
	 *  {@code competeNum} is fetched from the Competition options window ("# of games per competition").
	 *  
	 * @param swap {@code false} for 'Compete' and {@code true} for 'Swap Compete'
	 * @param both {@code true} for 'Compete Both' ({@code swap} is then irrelevant)
	 * @param xab	used only for reading parameter values from GUI members
	 * @param gb	needed for {@code competeBoth}
	 * @return the fitness of AgentX, which is in the range [-1.0,+1.0]: +1.0 if AgentX always wins,  
	 * 		   0.0 if always tie or if #win=#loose, and -1.0 if AgentX always looses.
	 */
	protected double competeBase(boolean swap, boolean both, XArenaButtons xab, GameBoard gb) {
		int competeNum = xab.winCompOptions.getNumGames();
		int numPlayers = gb.getStateObs().getNumPlayers();
		if (numPlayers!=2) {
			MessageBox.show(xab, 
					"Single/Swap Compete only available for 2-player games!", 
					"Error", JOptionPane.ERROR_MESSAGE);	
			return 0.0;
		}

		try {
			String AgentX = xab.getSelectedAgent(0);
			String AgentO = xab.getSelectedAgent(1);
			if (AgentX.equals("Human") | AgentO.equals("Human")) {
				MessageBox.show(xab, "No compete for agent Human", "Error", JOptionPane.ERROR_MESSAGE);
				return 0.0;
			} else {
				StateObservation startSO = gb.getDefaultStartState();  // empty board

				PlayAgent[] paVector = fetchAgents(xab);

				AgentBase.validTrainedAgents(paVector,numPlayers); // may throw RuntimeException
				
				PlayAgent[] qaVector = wrapAgents(paVector,xab,startSO);

				int verbose=1;

				if (both) {
					return competeBoth(qaVector[0],qaVector[1],startSO,competeNum,verbose,gb);
				} else {
					double[] res;
					if (swap) {
						res = compete(qaVector[1],qaVector[0],startSO,competeNum,verbose, null);
						System.out.println(Arrays.toString(res));
						return res[2] - res[0];
					} else {
						res = compete(qaVector[0],qaVector[1],startSO,competeNum,verbose, null);
						System.out.println(Arrays.toString(res));
						return res[0] - res[2];
					}
				}
			}
					
		} catch(RuntimeException ex) {
			MessageBox.show(xab, ex.getMessage(), "Error", JOptionPane.ERROR_MESSAGE);
			return 0;
		}
	} // competeBase

	public double singleCompete(XArenaButtons xab, GameBoard gb) {
		return this.competeBase(false, false, xab, gb);
	}

	public double swapCompete(XArenaButtons xab, GameBoard gb) {
		return this.competeBase(true, false, xab, gb);
	}

	public double bothCompete(XArenaButtons xab, GameBoard gb) {
		return this.competeBase(false, true, xab, gb);
	}

	/**
	 * This is an adapted version of {@link XArenaFuncs#competeBase(boolean, boolean, XArenaButtons, GameBoard) XArenaFuncs.competeBase()} 
	 * which is called for each match during a tournament. 
	 * The tournament parameters {@code dataTS} are added. Each match is currently fixed to one episode
	 * per match ({@code competeNum=1})
	 * 
	 * @param gb game board to play on
	 * @param xab used to access the param tabs for standard agents
	 * @param dataTS helper class containing data and settings for the next match in the tournament
	 * @return info who wins [(0/1/2) if (X/tie/O) wins] or error code (&gt;40)
	 */
	protected int singleCompeteBaseTS(GameBoard gb, XArenaButtons xab, TSGameDataTransfer dataTS) { 
		int competeNum = 1;//xab.winCompOptions.getNumGames(); | if value > 1 then adjust competeTS()!
		int numPlayers = gb.getStateObs().getNumPlayers();
		double[] c = {}; // win rate how often = [0]:agentX wins [1]: ties [2]: agentO wins

		try {
			String AgentX = dataTS.nextTeam[0].getAgentType();
			String AgentO = dataTS.nextTeam[1].getAgentType();
			if (AgentX.equals("Human") | AgentO.equals("Human")) {
				//MessageBox.show(xab, "No compete for agent Human", "Error", JOptionPane.ERROR_MESSAGE);
				//System.out.println(TAG+"ERROR :: No compete for agent Human, select different agent");
				throw new RuntimeException("No compete for agent Human, select different agent");
			} else {
				StateObservation startSO = gb.getDefaultStartState();  // empty board

				// manipulation of selected standard agent in XArenaButtons!
				xab.enableTournamentRemoteData(dataTS.nextTeam);

				// prepare agents
				PlayAgent[] paVector;
				PlayAgent[] qaVector;
				if (dataTS.nextTeam[0].isHddAgent() && dataTS.nextTeam[1].isHddAgent()) {
					paVector = new PlayAgent[2];
					paVector[0] = dataTS.nextTeam[0].getPlayAgent();
					paVector[1] = dataTS.nextTeam[1].getPlayAgent();
					AgentBase.validTrainedAgents(paVector,numPlayers); // may throw RuntimeException
//					OtherParams[] hddPar = new OtherParams[2];
//					hddPar[0] = new OtherParams();
//					hddPar[0].setWrapperNPly(paVector[0].getParOther().getWrapperNPly());
//					hddPar[1] = new OtherParams();
//					hddPar[1].setWrapperNPly(paVector[1].getParOther().getWrapperNPly());
//					qaVector = wrapAgents(paVector,hddPar,startSO);
					qaVector = wrapAgents(paVector,startSO, xab);
				} else {
					paVector = fetchAgents(xab);
					if (dataTS.nextTeam[0].isHddAgent()) {
						paVector[0] = dataTS.nextTeam[0].getPlayAgent();
					}
					if (dataTS.nextTeam[1].isHddAgent()) {
						paVector[1] = dataTS.nextTeam[1].getPlayAgent();
					}
					AgentBase.validTrainedAgents(paVector, numPlayers); // may throw RuntimeException
					qaVector = wrapAgents(paVector, xab, startSO);
				}

				//c = competeTS(qaVector[0], qaVector[1],        startSO, competeNum, 0, dataTS.nextTimes /*, dataTS.rndmStartMoves*/);
				//c = competeTS(qaVector[0], qaVector[1], dataTS.startSO, competeNum, 0, dataTS.nextTimes /*, dataTS.rndmStartMoves*/);
				c = compete(qaVector[0], qaVector[1], dataTS.startSO, competeNum, 0, dataTS.nextTimes /*, dataTS.rndmStartMoves*/);
				//System.out.println(Arrays.toString(c));

				xab.disableTournamentRemoteData();
			}

		} catch(RuntimeException ex) {
			MessageBox.show(xab, ex.getMessage(), "Error", JOptionPane.ERROR_MESSAGE);
			System.out.println(TAG+"ERROR :: RuntimeException :: "+ex.getMessage());
			return 43;
		}

		if (c[0]-c[2]>0.0)		// X wins (more often)
			return 0;
		if (c[0]-c[2]==0.0)		// tie or equal number of X and O wins
			return 1;
		if (c[0]-c[2]<1.0)		// O wins (more often)
			return 2;
		
		// we should never arrive here
		return 42;
	} // singleCompeteBaseTS

	/**
	 * Perform many (competitionNum) competitions between agents of type AgentX and agents 
	 * of type AgentO. The agents (if trainable) are trained anew before each competition. 
	 * @param silent
	 * @param xab		used only for reading parameter values from GUI members 
	 * @return			double[3], the percentage of episodes with X-win, tie, O-win 
	 * 					(averaged over all competitions) 
	 * @throws IOException 
	 */
	@Deprecated     // use Tournament System or MultiTrain instead
	public double[] multiCompete(boolean silent, XArenaButtons xab, GameBoard gb) 
			throws IOException {
		DecimalFormat frm = new DecimalFormat("#0.000");
		int verbose=1;
		int stopEval = 0;
		double[] winrate = new double[3];
		
		int numPlayers = gb.getStateObs().getNumPlayers();
		if (numPlayers!=2) {
			MessageBox.show(xab, 
					"Multi-Competition only available for 2-player games!", 
					"Error", JOptionPane.ERROR_MESSAGE);	
			return winrate;
		}
		
		try {
		// take settings from GUI xab
		String AgentX = xab.getSelectedAgent(0);  // has agent names as strings (as in Types.GUI_AGENT_LIST)
		String AgentO = xab.getSelectedAgent(1);
		int competeNum=xab.winCompOptions.getNumGames();
		int competitionNum=xab.winCompOptions.getNumCompetitions();
		int maxGameNum = Integer.parseInt(xab.GameNumT.getText());
		Evaluator m_evaluatorX=null;
		Evaluator m_evaluatorO=null;
		
		double optimCountX=0.0,optimCountO=0.0;
		double[][] winrateC = new double[competitionNum][3];
		double[][] evalC = new double[competitionNum][2];
		PlayAgent paX=null, paO=null, qa=null;
		if (verbose>0) System.out.println("Multi-Competition: "+competitionNum+" competitions with "
										 +competeNum+" episodes each, "+AgentX+" vs "+AgentO);

		if (AgentX.equals("Human") | AgentO.equals("Human")) {
			MessageBox.show(xab, 
					"No multiCompete for agent Human", 
					"Error", JOptionPane.ERROR_MESSAGE);
			return winrate;
		} 
		for (int c=0; c<competitionNum; c++) { // for-loop over competitions
			int player;

			// both agents are trained anew for the episodes of each competition
			try {
				paX = this.constructAgent(0,AgentX, xab);
				if (paX==null) throw new RuntimeException("Could not construct AgentX = " + AgentX);
			}  catch(RuntimeException e) 
			{
				MessageBox.show(xab, 
						e.getMessage(), 
						"Warning", JOptionPane.WARNING_MESSAGE);
				return winrate;			
			} 
			paX.setMaxGameNum(maxGameNum);
			paX.setGameNum(0);
			
			try {
				paO = this.constructAgent(1,AgentO, xab);
				if (paO==null) throw new RuntimeException("Could not construct AgentO = " + AgentO);
			}  catch(RuntimeException e) 
			{
				MessageBox.show(xab, 
						e.getMessage(), 
						"Warning", JOptionPane.WARNING_MESSAGE);
				return winrate;			
			} 
			paO.setMaxGameNum(maxGameNum);
			paO.setGameNum(0);
			
			// take the evaluator mode qem from the choice box 'Quick Eval Mode'
			// in tab 'Other pars':
			int qem = xab.oPar[0].getQuickEvalMode();
			m_evaluatorX = xab.m_game.makeEvaluator(paX,gb,stopEval,qem,1);
			
			if (paX.getAgentState()!=AgentState.TRAINED) {
				while (paX.getGameNum()<paX.getMaxGameNum())
				{							
					StateObservation so = soSelectStartState(gb,xab.oPar[0].useChooseStart01(), paX); 

					paX.trainAgent(so);
				}
				paX.setAgentState(AgentState.TRAINED);
			} 

			// construct 'qa' anew (possibly wrapped agent for eval)
			qa = wrapAgent(0, paX, new ParOther(xab.oPar[0]), new ParMaxN(xab.maxnParams[0]), gb.getStateObs());
			m_evaluatorX.eval(qa);
			evalC[c][0] = m_evaluatorX.getLastResult();
			optimCountX += evalC[c][0];
			
			qem = xab.oPar[1].getQuickEvalMode();
			m_evaluatorO = xab.m_game.makeEvaluator(paO,gb,stopEval,qem,1);
			
			if (paO.getAgentState()!=AgentState.TRAINED) {
				while (paO.getGameNum()<paO.getMaxGameNum())
				{							
					StateObservation so = soSelectStartState(gb,xab.oPar[1].useChooseStart01(), paO); 

					paO.trainAgent(so);
				}
				paO.setAgentState(AgentState.TRAINED);				
			} 

			// construct 'qa' anew (possibly wrapped agent for eval)
			qa = wrapAgent(1, paO, new ParOther(xab.oPar[1]), new ParMaxN(xab.maxnParams[1]), gb.getStateObs());
			m_evaluatorO.eval(qa);
			evalC[c][1] = m_evaluatorO.getLastResult();
			optimCountO += evalC[c][1];

			StateObservation startSO = gb.getDefaultStartState();  // empty board
			
			winrateC[c] = compete(paX,paO,startSO,competeNum,0, null);
			
			for (int i=0; i<3; i++) winrate[i] += winrateC[c][i];				
			if (!silent) {
				System.out.print(c + ": ");
				for (int i=0; i<3; i++) System.out.print(" "+frm.format(winrateC[c][i]));
				System.out.println();
			}
		} // for (c)
		
		for (int i=0; i<3; i++) winrate[i] = winrate[i]/competitionNum;
		if (!silent) {
			System.out.println("*** Competition results: ***");
			System.out.println("Agent X: Avg. "+m_evaluatorX.getPrintString()+": "+frm.format((double)optimCountX/competitionNum));
			System.out.println("Agent O: Avg. "+m_evaluatorO.getPrintString()+": "+frm.format((double)optimCountO/competitionNum));
		}

		//
		// write multiCompete statistics to "Arena.comp.csv"
		//
		String strDir = Types.GUI_DEFAULT_DIR_AGENT+"/"+xab.m_game.getGameName()+"/";
		String filename = "Arena.comp.csv";
		tools.Utils.checkAndCreateFolder(strDir);
		try {
			PrintWriter f; 
			f = new PrintWriter(new BufferedWriter(new FileWriter(strDir+filename)));
			
			// TODO: needs to be generalized to other agents but TDAgent: 
			for (int i=0; i<2; i++) {
				double alpha = xab.tdPar[i].getAlpha();
				double lambda = xab.tdPar[i].getLambda();
				f.println("alpha["+i+"]=" + alpha + ";  lambda["+i+"]=" + lambda + "; trained agents=" + competitionNum 
						  + ",  maxGameNum=" +maxGameNum);
			}
			f.print(AgentX);
			if (paX instanceof TDAgent) 
				f.print("("+((TDAgent)paX).getFeatmode()+")");
			f.print(" vs. ");  
			f.print(AgentO);
			if (paO instanceof TDAgent) 
				f.print("("+((TDAgent)paO).getFeatmode()+")");
			f.println(); f.println();
			f.println("C; X-win; tie; O-win; X success rate; O success rate");
			for (int c=0; c<competitionNum; c++) {
				f.print(c + "; ");
				for (int p=0; p<3; p++) f.print(winrateC[c][p] + "; ");
				for (int p=0; p<2; p++) f.print(evalC[c][p] + "; ");
				f.println();
			}
			f.println();
			f.println("Averages:");
			f.print(";");
			for (int p=0; p<3; p++) f.print(winrate[p] + "; ");
			f.print((double)optimCountX/competitionNum+"; ");
			f.print((double)optimCountO/competitionNum+"; ");
			f.println();
			f.close();
			System.out.println("multiCompete: Output written to " + strDir + filename);
		} catch (IOException e) {
			System.out.println("Could not write to "+strDir+filename+" in XArenaFuncs::multiCompete()");
		}
		
//		if (!silent) {
			System.out.print("Avg. win rates: ");
			for (int i=0; i<3; i++) System.out.print(frm.format(winrate[i])+"  ");
			System.out.println(" (X/Tie/O)");
//		}
		
		} catch(RuntimeException ex) {
			MessageBox.show(xab, 
					ex.getMessage(), 
					"Error", JOptionPane.ERROR_MESSAGE);
		}
		
		return winrate;
		
	} // multiCompete
	
	
	public String getLastMsg() {
		return lastMsg;
	}

}


